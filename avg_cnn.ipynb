{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16893422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm # Displays a progress bar\n",
    "from math import sqrt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torchvision import datasets, io, models, ops, transforms, utils\n",
    "from torch.utils.data import Dataset, Subset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "# from torchvision import datasets, io, models, ops, transforms, utils\n",
    "import os\n",
    "# import data as dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5dae4358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the CPU. Overall speed may be slowed down\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Using the GPU. You are good to go!\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"Using the CPU. Overall speed may be slowed down\")\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bba52881",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('output.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c9204605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>url</th>\n",
       "      <th>start_time</th>\n",
       "      <th>number_of_frames</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>label_proc</th>\n",
       "      <th>label_raw</th>\n",
       "      <th>label_notes</th>\n",
       "      <th>partition</th>\n",
       "      <th>signer</th>\n",
       "      <th>Label</th>\n",
       "      <th>start_l</th>\n",
       "      <th>end_l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>5936</td>\n",
       "      <td>deafvideo_5/scottnorby_6465</td>\n",
       "      <td>http://www.deafvideo.tv/815119</td>\n",
       "      <td>0:02:16.430000</td>\n",
       "      <td>8</td>\n",
       "      <td>640</td>\n",
       "      <td>360</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dev</td>\n",
       "      <td>105</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>5976</td>\n",
       "      <td>youtube_1/catherine_mackinnon_2799</td>\n",
       "      <td>https://www.youtube.com/watch?v=7myi959asB0</td>\n",
       "      <td>0:04:24.139000</td>\n",
       "      <td>7</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>kc</td>\n",
       "      <td>kc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dev</td>\n",
       "      <td>107</td>\n",
       "      <td>c</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>5979</td>\n",
       "      <td>youtube_1/catherine_mackinnon_2802</td>\n",
       "      <td>https://www.youtube.com/watch?v=7myi959asB0</td>\n",
       "      <td>0:04:45.022000</td>\n",
       "      <td>6</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>fd</td>\n",
       "      <td>fd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dev</td>\n",
       "      <td>107</td>\n",
       "      <td>d</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>5996</td>\n",
       "      <td>youtube_1/catherine_mackinnon_2819</td>\n",
       "      <td>https://www.youtube.com/watch?v=7myi959asB0</td>\n",
       "      <td>0:06:26.094000</td>\n",
       "      <td>8</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>co</td>\n",
       "      <td>co</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dev</td>\n",
       "      <td>107</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>6081</td>\n",
       "      <td>youtube_3/ben_bahan_4525</td>\n",
       "      <td>https://www.youtube.com/watch?v=7Y44OUbwthQ</td>\n",
       "      <td>0:01:05.733000</td>\n",
       "      <td>5</td>\n",
       "      <td>640</td>\n",
       "      <td>360</td>\n",
       "      <td>do</td>\n",
       "      <td>do</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dev</td>\n",
       "      <td>110</td>\n",
       "      <td>d</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6136</td>\n",
       "      <td>youtube_4/sean_berdy_5742</td>\n",
       "      <td>https://www.youtube.com/watch?v=2YLsJySoMLA</td>\n",
       "      <td>0:01:10.495000</td>\n",
       "      <td>5</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>co</td>\n",
       "      <td>co</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dev</td>\n",
       "      <td>113</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6137</td>\n",
       "      <td>youtube_4/sean_berdy_5743</td>\n",
       "      <td>https://www.youtube.com/watch?v=2YLsJySoMLA</td>\n",
       "      <td>0:01:11.795000</td>\n",
       "      <td>7</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>co</td>\n",
       "      <td>co</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dev</td>\n",
       "      <td>113</td>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6196</td>\n",
       "      <td>youtube_5/daniel_durant_5878</td>\n",
       "      <td>https://www.youtube.com/watch?v=20WRt_-Ls-k</td>\n",
       "      <td>0:02:38.745000</td>\n",
       "      <td>8</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>dtv</td>\n",
       "      <td>dtv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dev</td>\n",
       "      <td>116</td>\n",
       "      <td>d</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>6197</td>\n",
       "      <td>youtube_5/daniel_durant_5879</td>\n",
       "      <td>https://www.youtube.com/watch?v=20WRt_-Ls-k</td>\n",
       "      <td>0:02:49.700000</td>\n",
       "      <td>7</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>at</td>\n",
       "      <td>at</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dev</td>\n",
       "      <td>116</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6202</td>\n",
       "      <td>youtube_5/daniel_durant_5884</td>\n",
       "      <td>https://www.youtube.com/watch?v=20WRt_-Ls-k</td>\n",
       "      <td>0:03:33.010000</td>\n",
       "      <td>8</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>lexical</td>\n",
       "      <td>dev</td>\n",
       "      <td>116</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6204</td>\n",
       "      <td>youtube_5/daniel_durant_5886</td>\n",
       "      <td>https://www.youtube.com/watch?v=20WRt_-Ls-k</td>\n",
       "      <td>0:03:36.080000</td>\n",
       "      <td>6</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>lexical</td>\n",
       "      <td>dev</td>\n",
       "      <td>116</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6205</td>\n",
       "      <td>youtube_5/daniel_durant_5887</td>\n",
       "      <td>https://www.youtube.com/watch?v=20WRt_-Ls-k</td>\n",
       "      <td>0:03:41.565000</td>\n",
       "      <td>8</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>asl</td>\n",
       "      <td>asl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dev</td>\n",
       "      <td>116</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6229</td>\n",
       "      <td>youtube_5/jeffrey_spinale_6043</td>\n",
       "      <td>https://www.youtube.com/watch?v=LklvjsrdlUQ</td>\n",
       "      <td>0:01:50.728000</td>\n",
       "      <td>3</td>\n",
       "      <td>490</td>\n",
       "      <td>360</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dev</td>\n",
       "      <td>118</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6238</td>\n",
       "      <td>youtube_5/jeffrey_spinale_6052</td>\n",
       "      <td>https://www.youtube.com/watch?v=LklvjsrdlUQ</td>\n",
       "      <td>0:02:33.741000</td>\n",
       "      <td>7</td>\n",
       "      <td>490</td>\n",
       "      <td>360</td>\n",
       "      <td>apil</td>\n",
       "      <td>apil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dev</td>\n",
       "      <td>118</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6269</td>\n",
       "      <td>youtube_5/sean_berdy_6084</td>\n",
       "      <td>https://www.youtube.com/watch?v=7zxm_g9xCho</td>\n",
       "      <td>0:01:03.990000</td>\n",
       "      <td>8</td>\n",
       "      <td>640</td>\n",
       "      <td>360</td>\n",
       "      <td>at</td>\n",
       "      <td>at</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dev</td>\n",
       "      <td>113</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>6289</td>\n",
       "      <td>youtube_5/sean_berdy_6104</td>\n",
       "      <td>https://www.youtube.com/watch?v=7zxm_g9xCho</td>\n",
       "      <td>0:03:34.330000</td>\n",
       "      <td>4</td>\n",
       "      <td>640</td>\n",
       "      <td>360</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dev</td>\n",
       "      <td>113</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6304</td>\n",
       "      <td>youtube_5/sean_berdy_6119</td>\n",
       "      <td>https://www.youtube.com/watch?v=7zxm_g9xCho</td>\n",
       "      <td>0:04:38.970000</td>\n",
       "      <td>6</td>\n",
       "      <td>640</td>\n",
       "      <td>360</td>\n",
       "      <td>do</td>\n",
       "      <td>do</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dev</td>\n",
       "      <td>113</td>\n",
       "      <td>d</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6307</td>\n",
       "      <td>youtube_5/sean_berdy_6122</td>\n",
       "      <td>https://www.youtube.com/watch?v=7zxm_g9xCho</td>\n",
       "      <td>0:05:12.250000</td>\n",
       "      <td>8</td>\n",
       "      <td>640</td>\n",
       "      <td>360</td>\n",
       "      <td>fans</td>\n",
       "      <td>fans</td>\n",
       "      <td>fans?</td>\n",
       "      <td>dev</td>\n",
       "      <td>113</td>\n",
       "      <td>a</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6398</td>\n",
       "      <td>youtube_5/roberta_cordano_6132</td>\n",
       "      <td>https://www.youtube.com/watch?v=B7msla9Hkx0</td>\n",
       "      <td>0:00:41.845000</td>\n",
       "      <td>6</td>\n",
       "      <td>640</td>\n",
       "      <td>360</td>\n",
       "      <td>co</td>\n",
       "      <td>co</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dev</td>\n",
       "      <td>123</td>\n",
       "      <td>c</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                            filename  \\\n",
       "127        5936         deafvideo_5/scottnorby_6465   \n",
       "128        5976  youtube_1/catherine_mackinnon_2799   \n",
       "129        5979  youtube_1/catherine_mackinnon_2802   \n",
       "130        5996  youtube_1/catherine_mackinnon_2819   \n",
       "131        6081            youtube_3/ben_bahan_4525   \n",
       "132        6136           youtube_4/sean_berdy_5742   \n",
       "133        6137           youtube_4/sean_berdy_5743   \n",
       "134        6196        youtube_5/daniel_durant_5878   \n",
       "135        6197        youtube_5/daniel_durant_5879   \n",
       "136        6202        youtube_5/daniel_durant_5884   \n",
       "137        6204        youtube_5/daniel_durant_5886   \n",
       "138        6205        youtube_5/daniel_durant_5887   \n",
       "139        6229      youtube_5/jeffrey_spinale_6043   \n",
       "140        6238      youtube_5/jeffrey_spinale_6052   \n",
       "141        6269           youtube_5/sean_berdy_6084   \n",
       "142        6289           youtube_5/sean_berdy_6104   \n",
       "143        6304           youtube_5/sean_berdy_6119   \n",
       "144        6307           youtube_5/sean_berdy_6122   \n",
       "145        6398      youtube_5/roberta_cordano_6132   \n",
       "\n",
       "                                             url      start_time  \\\n",
       "127               http://www.deafvideo.tv/815119  0:02:16.430000   \n",
       "128  https://www.youtube.com/watch?v=7myi959asB0  0:04:24.139000   \n",
       "129  https://www.youtube.com/watch?v=7myi959asB0  0:04:45.022000   \n",
       "130  https://www.youtube.com/watch?v=7myi959asB0  0:06:26.094000   \n",
       "131  https://www.youtube.com/watch?v=7Y44OUbwthQ  0:01:05.733000   \n",
       "132  https://www.youtube.com/watch?v=2YLsJySoMLA  0:01:10.495000   \n",
       "133  https://www.youtube.com/watch?v=2YLsJySoMLA  0:01:11.795000   \n",
       "134  https://www.youtube.com/watch?v=20WRt_-Ls-k  0:02:38.745000   \n",
       "135  https://www.youtube.com/watch?v=20WRt_-Ls-k  0:02:49.700000   \n",
       "136  https://www.youtube.com/watch?v=20WRt_-Ls-k  0:03:33.010000   \n",
       "137  https://www.youtube.com/watch?v=20WRt_-Ls-k  0:03:36.080000   \n",
       "138  https://www.youtube.com/watch?v=20WRt_-Ls-k  0:03:41.565000   \n",
       "139  https://www.youtube.com/watch?v=LklvjsrdlUQ  0:01:50.728000   \n",
       "140  https://www.youtube.com/watch?v=LklvjsrdlUQ  0:02:33.741000   \n",
       "141  https://www.youtube.com/watch?v=7zxm_g9xCho  0:01:03.990000   \n",
       "142  https://www.youtube.com/watch?v=7zxm_g9xCho  0:03:34.330000   \n",
       "143  https://www.youtube.com/watch?v=7zxm_g9xCho  0:04:38.970000   \n",
       "144  https://www.youtube.com/watch?v=7zxm_g9xCho  0:05:12.250000   \n",
       "145  https://www.youtube.com/watch?v=B7msla9Hkx0  0:00:41.845000   \n",
       "\n",
       "     number_of_frames  width  height label_proc label_raw label_notes  \\\n",
       "127                 8    640     360        all       all         NaN   \n",
       "128                 7   1280     720         kc        kc         NaN   \n",
       "129                 6   1280     720         fd       fd          NaN   \n",
       "130                 8   1280     720         co        co         NaN   \n",
       "131                 5    640     360         do        do         NaN   \n",
       "132                 5   1280     720         co        co         NaN   \n",
       "133                 7   1280     720         co        co         NaN   \n",
       "134                 8   1280     720        dtv       dtv         NaN   \n",
       "135                 7   1280     720         at        at         NaN   \n",
       "136                 8   1280     720        all      all      lexical   \n",
       "137                 6   1280     720        all      all      lexical   \n",
       "138                 8   1280     720        asl       asl         NaN   \n",
       "139                 3    490     360        all       all         NaN   \n",
       "140                 7    490     360       apil      apil         NaN   \n",
       "141                 8    640     360         at        at         NaN   \n",
       "142                 4    640     360        all       all         NaN   \n",
       "143                 6    640     360         do        do         NaN   \n",
       "144                 8    640     360       fans      fans       fans?   \n",
       "145                 6    640     360         co        co         NaN   \n",
       "\n",
       "    partition  signer Label  start_l  end_l  \n",
       "127       dev     105     a        1      1  \n",
       "128       dev     107     c        4      5  \n",
       "129       dev     107     d        3      6  \n",
       "130       dev     107     c        1      3  \n",
       "131       dev     110     d        1      3  \n",
       "132       dev     113     c        1      2  \n",
       "133       dev     113     c        1      1  \n",
       "134       dev     116     d        1      3  \n",
       "135       dev     116     a        1      1  \n",
       "136       dev     116     a        1      1  \n",
       "137       dev     116     a        1      1  \n",
       "138       dev     116     a        1      3  \n",
       "139       dev     118     a        1      2  \n",
       "140       dev     118     a        1      1  \n",
       "141       dev     113     a        1      3  \n",
       "142       dev     113     a        1      1  \n",
       "143       dev     113     d        1      3  \n",
       "144       dev     113     a        5      5  \n",
       "145       dev     123     c        2      5  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['partition']=='dev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ec47fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_exists(filename):\n",
    "    folders = ['avg_dev', 'avg_test', 'avg_train']\n",
    "    filename=filename.replace('/','_')+\".png\"\n",
    "    print(filename)\n",
    "    for folder in folders:\n",
    "        if os.path.exists(os.path.join(folder, filename)):\n",
    "            return True\n",
    "    return False\n",
    "class HandSignDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, partition, transform=None):\n",
    "        self.df = pd.read_csv(csv_file, delimiter=';')\n",
    "        self.df = self.df[self.df['partition'] == partition]\n",
    "    \n",
    "        self.df=self.df[self.df['filename'].apply(file_exists)]\n",
    "\n",
    "    # define a function to check if a file exists in any of the folders\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.df.iloc[idx, self.df.columns.get_loc('filename')]\n",
    "        filename_img = self.df.iloc[idx, self.df.columns.get_loc('filename')].replace('/','_')\n",
    "        label = self.df.iloc[idx, self.df.columns.get_loc('Label')]\n",
    "        \n",
    "\n",
    "        image_path = os.path.join(self.root_dir, filename_img+\".png\")\n",
    "        bbox_path = os.path.join(\"BBox\", filename, \"0000.txt\")\n",
    "        \n",
    "        try:\n",
    "            with open(bbox_path) as f:\n",
    "                bbox_info = f.readline().split(',')\n",
    "                print(bbox_info)\n",
    "            x0, y0, x1, y1, _ = bbox_info\n",
    "            x0, y0, x1, y1 = int(x0), int(y0), int(x1), int(y1)\n",
    "\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = image.crop((x0, y0, x1, y1))\n",
    "        except FileNotFoundError:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        if(np.sum(image)==0):\n",
    "            print('ALL ZERO')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        print('image: ',filename_img,\"type and image:\",type(image),image)\n",
    "    \n",
    "        print('label: ',type(label),label)\n",
    "#         utils.save_image(img, f\"/ImageOutput/{filename_img}_T.png\")\n",
    "        # img1 = img1.numpy() # TypeError: tensor or list of tensors expected, got <class 'numpy.ndarray'>\n",
    "#         save_image(img, filename +'_T.png')\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9d6e6f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "youtube_5_roberta_cordano_6136.png\n",
      "youtube_5_roberta_cordano_6149.png\n",
      "youtube_5_roberta_cordano_6154.png\n",
      "youtube_6_roberta_cordano_6477.png\n",
      "youtube_6_roberta_cordano_6480.png\n",
      "deafvideo_3_kathyfans_3459.png\n",
      "deafvideo_3_kathyfans_3473.png\n",
      "deafvideo_3_kazio63_3504.png\n",
      "deafvideo_3_kazio63_3505.png\n",
      "deafvideo_3_kazio63_3507.png\n",
      "deafvideo_3_kazio63_3508.png\n",
      "youtube_1_joel_garcia_2863.png\n",
      "youtube_2_nick_martin_3419.png\n",
      "youtube_2_nick_martin_3435.png\n",
      "youtube_5_david_o._reynolds_5819.png\n",
      "youtube_5_keith_nolan_6011.png\n",
      "youtube_5_keith_nolan_6012.png\n",
      "deafvideo_4_floridaaahh_5724.png\n",
      "deafvideo_4_floridaaahh_5725.png\n",
      "misc_2_genie_gertz_2892.png\n",
      "aslized_joseph_hill_1761.png\n",
      "aslized_mj_bienvenu_7057.png\n",
      "aslized_mj_bienvenu_7068.png\n",
      "aslthat_joseph_wheeler_0589.png\n",
      "deafvideo_1_taylerade_1231.png\n",
      "deafvideo_1_taylerade_1275.png\n",
      "deafvideo_1_wesley_arey_0913.png\n",
      "deafvideo_1_wesley_arey_0915.png\n",
      "deafvideo_2_dsport06_1618.png\n",
      "deafvideo_2_dsport06_1625.png\n",
      "deafvideo_2_dsport06_1627.png\n",
      "deafvideo_3_deafaynrand_4489.png\n",
      "deafvideo_4_dsport06_4889.png\n",
      "deafvideo_4_dsport06_4890.png\n",
      "deafvideo_4_dsport06_4896.png\n",
      "deafvideo_4_dsport06_4905.png\n",
      "deafvideo_4_dsport06_4907.png\n",
      "deafvideo_4_dsport06_4909.png\n",
      "deafvideo_4_dsport06_4926.png\n",
      "deafvideo_4_dsport06_4928.png\n",
      "deafvideo_4_dsport06_4931.png\n",
      "deafvideo_4_dsport06_4932.png\n",
      "deafvideo_4_dsport06_4948.png\n",
      "deafvideo_4_mattref2005_5169.png\n",
      "deafvideo_4_mattref2005_5199.png\n",
      "deafvideo_4_mattref2005_5202.png\n",
      "deafvideo_4_mattref2005_5203.png\n",
      "deafvideo_4_shebaby_4997.png\n",
      "deafvideo_4_taylerade_5628.png\n",
      "deafvideo_4_taylerade_5664.png\n",
      "deafvideo_5_archie_6981.png\n",
      "misc_1_carol_padden_1908.png\n",
      "misc_1_carol_padden_1911.png\n",
      "misc_1_carol_padden_1912.png\n",
      "misc_1_carol_padden_1974.png\n",
      "misc_1_carol_padden_2002.png\n",
      "misc_1_carol_padden_2012.png\n",
      "misc_1_carol_padden_2190.png\n",
      "misc_2_aidan_mack_3176.png\n",
      "misc_2_aidan_mack_3183.png\n",
      "misc_2_aidan_mack_3186.png\n",
      "misc_2_aidan_mack_3196.png\n",
      "misc_2_aidan_mack_3201.png\n",
      "misc_2_jehanne_mccullough_3898.png\n",
      "misc_2_marlon_kuntze_2473.png\n",
      "misc_2_marlon_kuntze_2575.png\n",
      "misc_2_marlon_kuntze_2604.png\n",
      "misc_2_marlon_kuntze_2606.png\n",
      "misc_2_marlon_kuntze_2653.png\n",
      "misc_2_marlon_kuntze_2673.png\n",
      "misc_2_marlon_kuntze_2682.png\n",
      "misc_2_marlon_kuntze_2721.png\n",
      "misc_2_marlon_kuntze_2726.png\n",
      "misc_2_ryan_commerson_4985.png\n",
      "misc_2_ryan_commerson_4988.png\n",
      "misc_2_ryan_commerson_4989.png\n",
      "misc_2_ryan_commerson_4992.png\n",
      "youtube_1_alexandria_wailes_1870.png\n",
      "youtube_1_alexandria_wailes_1887.png\n",
      "youtube_1_jackie_roth_2986.png\n",
      "youtube_1_jackie_roth_3001.png\n",
      "youtube_1_jackie_roth_3003.png\n",
      "youtube_1_jackie_roth_3021.png\n",
      "youtube_1_leah_katz-hernandez_2848.png\n",
      "youtube_2_alex_abenchuchan_3354.png\n",
      "youtube_2_alex_abenchuchan_3932.png\n",
      "youtube_2_chickadee_3529.png\n",
      "youtube_2_tia_albert_3721.png\n",
      "youtube_2_tia_albert_3750.png\n",
      "youtube_2_tia_albert_3803.png\n",
      "youtube_2_tia_albert_3818.png\n",
      "youtube_3_alex_abenchuchan_4018.png\n",
      "youtube_3_alex_abenchuchan_4089.png\n",
      "youtube_3_alex_abenchuchan_4127.png\n",
      "youtube_3_alex_abenchuchan_4313.png\n",
      "youtube_3_raymond_merritt_4716.png\n",
      "youtube_3_raymond_merritt_4774.png\n",
      "youtube_4_alex_abenchuchan_5403.png\n",
      "youtube_4_alex_abenchuchan_5441.png\n",
      "youtube_4_alex_abenchuchan_5452.png\n",
      "youtube_4_alex_abenchuchan_5457.png\n",
      "youtube_4_alex_abenchuchan_5467.png\n",
      "youtube_4_evelina_gaina_4869.png\n",
      "youtube_4_evelina_gaina_4881.png\n",
      "youtube_4_raymond_merritt_4811.png\n",
      "youtube_4_raymond_merritt_4818.png\n",
      "youtube_4_raymond_merritt_4819.png\n",
      "youtube_4_raymond_merritt_4832.png\n",
      "youtube_4_raymond_merritt_4833.png\n",
      "youtube_4_raymond_merritt_4834.png\n",
      "youtube_4_raymond_merritt_4835.png\n",
      "youtube_4_raymond_merritt_4839.png\n",
      "youtube_4_raymond_merritt_4841.png\n",
      "youtube_4_raymond_merritt_4848.png\n",
      "youtube_4_tawny_holmes_5603.png\n",
      "youtube_4_tawny_holmes_5604.png\n",
      "youtube_4_tawny_holmes_5606.png\n",
      "youtube_5_christine_sun_kim_5900.png\n",
      "youtube_5_ryan_lane_5874.png\n",
      "youtube_5_tawny_holmes_6166.png\n",
      "youtube_5_tawny_holmes_6167.png\n",
      "youtube_5_tawny_holmes_6169.png\n",
      "youtube_5_tawny_holmes_6171.png\n",
      "youtube_5_tawny_holmes_6173.png\n",
      "youtube_5_tawny_holmes_6177.png\n",
      "youtube_5_tawny_holmes_6180.png\n",
      "youtube_5_tawny_holmes_6189.png\n",
      "youtube_5_tawny_holmes_6196.png\n",
      "youtube_5_tawny_holmes_6211.png\n",
      "youtube_5_tawny_holmes_6242.png\n",
      "youtube_6_marlee_matlin_6709.png\n",
      "youtube_6_marlee_matlin_6863.png\n",
      "youtube_6_tom_humphries_6617.png\n",
      "youtube_6_tom_humphries_6619.png\n",
      "youtube_6_tom_humphries_6642.png\n",
      "youtube_6_tom_humphries_6656.png\n",
      "youtube_6_tom_humphries_6673.png\n",
      "youtube_6_tom_humphries_6681.png\n",
      "aslized_felicia_williams_7027.png\n",
      "aslized_felicia_williams_7036.png\n",
      "aslized_felicia_williams_7038.png\n",
      "aslized_felicia_williams_7040.png\n",
      "deafvideo_4_zainab_5665.png\n",
      "deafvideo_4_zainab_5679.png\n",
      "deafvideo_3_damien23_3602.png\n",
      "deafvideo_3_otismhill82_4621.png\n",
      "deafvideo_3_yellowbirdie84_4676.png\n",
      "deafvideo_5_scottnorby_6465.png\n",
      "youtube_1_catherine_mackinnon_2799.png\n",
      "youtube_1_catherine_mackinnon_2802.png\n",
      "youtube_1_catherine_mackinnon_2819.png\n",
      "youtube_3_ben_bahan_4525.png\n",
      "youtube_4_sean_berdy_5742.png\n",
      "youtube_4_sean_berdy_5743.png\n",
      "youtube_5_daniel_durant_5878.png\n",
      "youtube_5_daniel_durant_5879.png\n",
      "youtube_5_daniel_durant_5884.png\n",
      "youtube_5_daniel_durant_5886.png\n",
      "youtube_5_daniel_durant_5887.png\n",
      "youtube_5_jeffrey_spinale_6043.png\n",
      "youtube_5_jeffrey_spinale_6052.png\n",
      "youtube_5_sean_berdy_6084.png\n",
      "youtube_5_sean_berdy_6104.png\n",
      "youtube_5_sean_berdy_6119.png\n",
      "youtube_5_sean_berdy_6122.png\n",
      "youtube_5_roberta_cordano_6132.png\n"
     ]
    }
   ],
   "source": [
    "# Define the transformation(s) to be applied to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                          std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Call the create_dataset function to create a PyTorch dataset\n",
    "test_dataset = HandSignDataset(csv_file='output.csv', root_dir='avg_test', partition='test',transform=transform)\n",
    "train_dataset = HandSignDataset(csv_file='output.csv', root_dir='avg_train', partition='train',transform=transform)\n",
    "val_dataset = HandSignDataset(csv_file='output.csv', root_dir='avg_dev', partition='dev',transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0400812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "# Create a data loader for the dataset\n",
    "\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, collate_fn=custom_collate)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "51ca7db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image:  deafvideo_2_dsport06_1625 type and image: <class 'torch.Tensor'> tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "label:  <class 'str'> a\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/ImageOutput/deafvideo_2_dsport06_1625_T.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [89]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m trainloader:\n\u001b[1;32m      2\u001b[0m     img\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m     label\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[0;32mIn [86]\u001b[0m, in \u001b[0;36mHandSignDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage: \u001b[39m\u001b[38;5;124m'\u001b[39m,filename_img,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype and image:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mtype\u001b[39m(image),image)\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel: \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mtype\u001b[39m(label),label)\n\u001b[0;32m---> 52\u001b[0m         \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/ImageOutput/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfilename_img\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_T.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;66;03m# img1 = img1.numpy() # TypeError: tensor or list of tensors expected, got <class 'numpy.ndarray'>\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#         save_image(img, filename +'_T.png')\u001b[39;00m\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/utils.py:151\u001b[0m, in \u001b[0;36msave_image\u001b[0;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m ndarr \u001b[38;5;241m=\u001b[39m grid\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39madd_(\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39muint8)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    150\u001b[0m im \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(ndarr)\n\u001b[0;32m--> 151\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/PIL/Image.py:2209\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2207\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2209\u001b[0m         fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw+b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2211\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2212\u001b[0m     save_handler(\u001b[38;5;28mself\u001b[39m, fp, filename)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/ImageOutput/deafvideo_2_dsport06_1625_T.png'"
     ]
    }
   ],
   "source": [
    "for batch in trainloader:\n",
    "    img=batch[0]\n",
    "    label=batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a8d9134b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your network:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 14, 14]             416\n",
      "         MaxPool2d-2             [-1, 16, 7, 7]               0\n",
      "            Conv2d-3             [-1, 64, 4, 4]          25,664\n",
      "         MaxPool2d-4             [-1, 64, 2, 2]               0\n",
      "            Conv2d-5              [-1, 8, 1, 1]          12,808\n",
      "            Linear-6                   [-1, 10]              90\n",
      "================================================================\n",
      "Total params: 38,978\n",
      "Trainable params: 38,978\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.04\n",
      "Params size (MB): 0.15\n",
      "Estimated Total Size (MB): 0.19\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ##############################################################################\n",
    "        # TODO: Design your own network, define layers here.                          #\n",
    "        # Here We provide a sample of two-layer fc network from HW4 Part3.           #\n",
    "        # Your solution, however, should contain convolutional layers.               #\n",
    "        # Refer to PyTorch documentations of torch.nn to pick your layers.           #\n",
    "        # (https://pytorch.org/docs/stable/nn.html)                                  #\n",
    "        # Some common choices: Linear, Conv2d, ReLU, MaxPool2d, AvgPool2d, Dropout   #\n",
    "        # If you have many layers, use nn.Sequential() to simplify your code         #\n",
    "        ##############################################################################\n",
    "        # from 28x28 input image to hidden layer of size 256\n",
    "#         self.fc1 = nn.Linear(28*28, 8) \n",
    "        self.conv1 =nn.Conv2d(in_channels = 1,out_channels = 16,padding = 2, kernel_size = (5,5),stride = (2,2))\n",
    "        self.pool =nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        self.conv2 =nn.Conv2d(in_channels = 16,out_channels = 64, padding = 2,kernel_size = (5,5),stride = (2,2))\n",
    "        self.conv3 =nn.Conv2d(in_channels = 64,out_channels = 8, padding = 2,kernel_size = (5,5),stride = (2,2))\n",
    "        self.fc1 = nn.Linear(in_features = 8, out_features = 10) \n",
    "\n",
    "        self.init_weights()\n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize all model parameters (weights and biases) in all layers to desired distributions\"\"\"\n",
    "\n",
    "        torch.manual_seed(42)\n",
    "        for conv in [self.conv1, self.conv2, self.conv3]:\n",
    "            C_in = conv.weight.size(1)\n",
    "            nn.init.normal_(conv.weight, 0.0, 1 / sqrt(5 * 5 * C_in))\n",
    "            nn.init.constant_(conv.bias, 0.0)\n",
    "\n",
    "        ## TODO: initialize the parameters for [self.fc1]\n",
    "        nn.init.normal_(self.fc1.weight, 0.0, sqrt(1/self.fc1.weight.size(1)))\n",
    "        nn.init.constant_(self.fc1.bias, 0.0)\n",
    "        ##\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ##############################################################################\n",
    "        # TODO: Design your own network, implement forward pass here                 # \n",
    "        ##############################################################################\n",
    "        \n",
    "        x = x.to(device)\n",
    "        # Flatten each image in the batch\n",
    "        z= self.pool(F.relu(self.conv1(x)))\n",
    "        z= self.pool(F.relu(self.conv2(z)))\n",
    "        z = F.relu(self.conv3(z))\n",
    "\n",
    "        # z=z.permute(*torch.arange(z.ndim - 1, -1, -1))\n",
    "\n",
    "        z=torch.flatten(z, start_dim=1)\n",
    "\n",
    "        # print(\"after resize: \",z.shape)\n",
    "        z=self.fc1(z)\n",
    "  \n",
    "  \n",
    "        ##\n",
    "        \n",
    "\n",
    "        # The loss layer will be applied outside Network class\n",
    "        return z\n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "\n",
    "model = Network().to(device)\n",
    "criterion = nn.CrossEntropyLoss() # Specify the loss layer\n",
    "print('Your network:')\n",
    "print(summary(model, (1,28,28), device=device)) # visualize your model\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Modify the lines below to experiment with different optimizers,      #\n",
    "# parameters (such as learning rate) and number of epochs.                   #\n",
    "##############################################################################\n",
    "# Set up optimization hyperparameters\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "num_epoch = 20  # TODO: Choose an appropriate number of training epochs\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                       weight_decay=weight_decay) # Try different optimizers\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55421ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "-----------------Epoch = 1-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image:  youtube_2_tia_albert_3803 type and image: <class 'torch.Tensor'> tensor([[[0.6941, 0.6902, 0.6941,  ..., 0.2471, 0.2588, 0.2471],\n",
      "         [0.6902, 0.6902, 0.6980,  ..., 0.2431, 0.2588, 0.2471],\n",
      "         [0.6902, 0.6941, 0.6980,  ..., 0.2471, 0.2627, 0.2510],\n",
      "         ...,\n",
      "         [0.7020, 0.6980, 0.6235,  ..., 0.2863, 0.2706, 0.2392],\n",
      "         [0.6980, 0.6941, 0.6196,  ..., 0.2824, 0.2706, 0.2353],\n",
      "         [0.6941, 0.6902, 0.6157,  ..., 0.2824, 0.2706, 0.2353]],\n",
      "\n",
      "        [[0.7529, 0.7529, 0.7608,  ..., 0.2000, 0.2157, 0.2314],\n",
      "         [0.7529, 0.7569, 0.7647,  ..., 0.1961, 0.2157, 0.2275],\n",
      "         [0.7608, 0.7608, 0.7686,  ..., 0.1961, 0.2157, 0.2275],\n",
      "         ...,\n",
      "         [0.7569, 0.7529, 0.6745,  ..., 0.1922, 0.1961, 0.2039],\n",
      "         [0.7529, 0.7490, 0.6706,  ..., 0.1922, 0.2000, 0.2039],\n",
      "         [0.7490, 0.7451, 0.6667,  ..., 0.1922, 0.2000, 0.2039]],\n",
      "\n",
      "        [[0.7922, 0.7922, 0.7882,  ..., 0.1333, 0.1490, 0.1765],\n",
      "         [0.7961, 0.8000, 0.7922,  ..., 0.1333, 0.1490, 0.1804],\n",
      "         [0.8039, 0.8078, 0.8000,  ..., 0.1294, 0.1490, 0.1804],\n",
      "         ...,\n",
      "         [0.7647, 0.6824, 0.6196,  ..., 0.0902, 0.0902, 0.1373],\n",
      "         [0.7608, 0.6824, 0.6157,  ..., 0.0902, 0.0902, 0.1373],\n",
      "         [0.7569, 0.6784, 0.6118,  ..., 0.0902, 0.0902, 0.1373]]])\n",
      "label:  <class 'str'> a\n",
      "image:  youtube_4_tawny_holmes_5606 type and image: <class 'torch.Tensor'> tensor([[[0.7373, 0.5490, 0.4627,  ..., 0.5608, 0.6078, 0.6431],\n",
      "         [0.7647, 0.6275, 0.5451,  ..., 0.5686, 0.6157, 0.6510],\n",
      "         [0.7686, 0.7098, 0.6549,  ..., 0.5725, 0.6157, 0.6510],\n",
      "         ...,\n",
      "         [0.6706, 0.6784, 0.6824,  ..., 0.1451, 0.1373, 0.1216],\n",
      "         [0.6745, 0.6824, 0.6824,  ..., 0.1608, 0.1608, 0.1529],\n",
      "         [0.6667, 0.6745, 0.6745,  ..., 0.1686, 0.1686, 0.1608]],\n",
      "\n",
      "        [[0.6863, 0.5137, 0.4314,  ..., 0.5765, 0.6118, 0.6431],\n",
      "         [0.7255, 0.5961, 0.5137,  ..., 0.5843, 0.6196, 0.6471],\n",
      "         [0.7412, 0.6784, 0.6235,  ..., 0.5843, 0.6235, 0.6510],\n",
      "         ...,\n",
      "         [0.6706, 0.6784, 0.6824,  ..., 0.1451, 0.1373, 0.1216],\n",
      "         [0.6745, 0.6824, 0.6824,  ..., 0.1608, 0.1608, 0.1529],\n",
      "         [0.6667, 0.6745, 0.6745,  ..., 0.1686, 0.1686, 0.1647]],\n",
      "\n",
      "        [[0.6118, 0.4235, 0.3333,  ..., 0.5765, 0.6039, 0.6353],\n",
      "         [0.6353, 0.4980, 0.4118,  ..., 0.5804, 0.6118, 0.6392],\n",
      "         [0.6275, 0.5725, 0.5216,  ..., 0.5804, 0.6078, 0.6392],\n",
      "         ...,\n",
      "         [0.7020, 0.7098, 0.7137,  ..., 0.1804, 0.1686, 0.1529],\n",
      "         [0.7059, 0.7137, 0.7137,  ..., 0.1961, 0.1922, 0.1804],\n",
      "         [0.6980, 0.7059, 0.7059,  ..., 0.2039, 0.2000, 0.1882]]])\n",
      "label:  <class 'str'> d\n",
      "image:  youtube_3_alex_abenchuchan_4127 type and image: <class 'torch.Tensor'> tensor([[[0.1686, 0.1686, 0.1686,  ..., 0.1882, 0.1882, 0.1843],\n",
      "         [0.1686, 0.1725, 0.1725,  ..., 0.1882, 0.1882, 0.1843],\n",
      "         [0.1686, 0.1725, 0.1725,  ..., 0.1882, 0.1882, 0.1843],\n",
      "         ...,\n",
      "         [0.1216, 0.1216, 0.1216,  ..., 0.1451, 0.1451, 0.1451],\n",
      "         [0.1216, 0.1216, 0.1216,  ..., 0.1451, 0.1451, 0.1451],\n",
      "         [0.1216, 0.1216, 0.1216,  ..., 0.1451, 0.1451, 0.1451]],\n",
      "\n",
      "        [[0.1686, 0.1686, 0.1686,  ..., 0.1882, 0.1882, 0.1843],\n",
      "         [0.1686, 0.1725, 0.1725,  ..., 0.1882, 0.1882, 0.1843],\n",
      "         [0.1686, 0.1725, 0.1725,  ..., 0.1882, 0.1882, 0.1843],\n",
      "         ...,\n",
      "         [0.1216, 0.1216, 0.1216,  ..., 0.1451, 0.1451, 0.1451],\n",
      "         [0.1216, 0.1216, 0.1216,  ..., 0.1451, 0.1451, 0.1451],\n",
      "         [0.1216, 0.1216, 0.1216,  ..., 0.1451, 0.1451, 0.1451]],\n",
      "\n",
      "        [[0.1686, 0.1686, 0.1686,  ..., 0.1882, 0.1882, 0.1843],\n",
      "         [0.1686, 0.1725, 0.1725,  ..., 0.1882, 0.1882, 0.1843],\n",
      "         [0.1686, 0.1725, 0.1725,  ..., 0.1882, 0.1882, 0.1843],\n",
      "         ...,\n",
      "         [0.1216, 0.1216, 0.1216,  ..., 0.1451, 0.1451, 0.1451],\n",
      "         [0.1216, 0.1216, 0.1216,  ..., 0.1451, 0.1451, 0.1451],\n",
      "         [0.1216, 0.1216, 0.1216,  ..., 0.1451, 0.1451, 0.1451]]])\n",
      "label:  <class 'str'> d\n",
      "image:  youtube_3_raymond_merritt_4774 type and image: <class 'torch.Tensor'> tensor([[[0.5294, 0.5255, 0.5294,  ..., 0.3569, 0.3608, 0.3647],\n",
      "         [0.5333, 0.5333, 0.5333,  ..., 0.3569, 0.3608, 0.3647],\n",
      "         [0.5333, 0.5333, 0.5333,  ..., 0.3647, 0.3686, 0.3725],\n",
      "         ...,\n",
      "         [0.6275, 0.6275, 0.6000,  ..., 0.4196, 0.3647, 0.3725],\n",
      "         [0.6510, 0.6078, 0.5882,  ..., 0.3725, 0.3804, 0.4353],\n",
      "         [0.6549, 0.6078, 0.5333,  ..., 0.3882, 0.4392, 0.4588]],\n",
      "\n",
      "        [[0.3647, 0.3608, 0.3647,  ..., 0.2980, 0.3059, 0.3098],\n",
      "         [0.3686, 0.3686, 0.3686,  ..., 0.2980, 0.3059, 0.3098],\n",
      "         [0.3686, 0.3686, 0.3686,  ..., 0.3020, 0.3098, 0.3176],\n",
      "         ...,\n",
      "         [0.6078, 0.5882, 0.5569,  ..., 0.2078, 0.1882, 0.2118],\n",
      "         [0.6353, 0.5608, 0.5373,  ..., 0.1961, 0.2314, 0.2902],\n",
      "         [0.6275, 0.5451, 0.4588,  ..., 0.2235, 0.2824, 0.3137]],\n",
      "\n",
      "        [[0.2157, 0.2078, 0.2078,  ..., 0.2471, 0.2588, 0.2667],\n",
      "         [0.2118, 0.2118, 0.2078,  ..., 0.2471, 0.2588, 0.2667],\n",
      "         [0.2118, 0.2118, 0.2078,  ..., 0.2471, 0.2588, 0.2706],\n",
      "         ...,\n",
      "         [0.6431, 0.6118, 0.5765,  ..., 0.0627, 0.0745, 0.1059],\n",
      "         [0.6706, 0.5843, 0.5490,  ..., 0.0824, 0.1137, 0.1529],\n",
      "         [0.6627, 0.5569, 0.4706,  ..., 0.1255, 0.1529, 0.1451]]])\n",
      "label:  <class 'str'> c\n",
      "image:  youtube_2_alex_abenchuchan_3932 type and image: <class 'torch.Tensor'> tensor([[[0.2039, 0.2078, 0.2118,  ..., 0.2118, 0.2118, 0.2118],\n",
      "         [0.2078, 0.2078, 0.2157,  ..., 0.2118, 0.2118, 0.2118],\n",
      "         [0.2078, 0.2118, 0.2157,  ..., 0.2118, 0.2118, 0.2118],\n",
      "         ...,\n",
      "         [0.1333, 0.1333, 0.1333,  ..., 0.1529, 0.1529, 0.1529],\n",
      "         [0.1333, 0.1333, 0.1333,  ..., 0.1529, 0.1529, 0.1529],\n",
      "         [0.1294, 0.1294, 0.1333,  ..., 0.1529, 0.1529, 0.1529]],\n",
      "\n",
      "        [[0.2078, 0.2118, 0.2157,  ..., 0.2157, 0.2157, 0.2157],\n",
      "         [0.2118, 0.2118, 0.2196,  ..., 0.2157, 0.2157, 0.2157],\n",
      "         [0.2118, 0.2157, 0.2196,  ..., 0.2157, 0.2157, 0.2157],\n",
      "         ...,\n",
      "         [0.1373, 0.1373, 0.1373,  ..., 0.1529, 0.1529, 0.1529],\n",
      "         [0.1373, 0.1373, 0.1373,  ..., 0.1529, 0.1529, 0.1529],\n",
      "         [0.1333, 0.1333, 0.1373,  ..., 0.1529, 0.1529, 0.1529]],\n",
      "\n",
      "        [[0.1882, 0.1922, 0.1961,  ..., 0.1961, 0.1961, 0.1961],\n",
      "         [0.1922, 0.1922, 0.2000,  ..., 0.1961, 0.1961, 0.1961],\n",
      "         [0.1922, 0.1961, 0.2000,  ..., 0.1961, 0.1961, 0.1961],\n",
      "         ...,\n",
      "         [0.1176, 0.1176, 0.1176,  ..., 0.1373, 0.1373, 0.1373],\n",
      "         [0.1176, 0.1176, 0.1176,  ..., 0.1373, 0.1373, 0.1373],\n",
      "         [0.1137, 0.1137, 0.1176,  ..., 0.1373, 0.1373, 0.1373]]])\n",
      "label:  <class 'str'> b\n",
      "image:  misc_2_aidan_mack_3196 type and image: <class 'torch.Tensor'> tensor([[[0.3333, 0.3529, 0.5373,  ..., 0.6745, 0.7020, 0.7451],\n",
      "         [0.3804, 0.3686, 0.5294,  ..., 0.7059, 0.7255, 0.7569],\n",
      "         [0.4314, 0.3765, 0.5098,  ..., 0.7373, 0.7451, 0.7490],\n",
      "         ...,\n",
      "         [0.9569, 0.9608, 0.9608,  ..., 0.5804, 0.5490, 0.5098],\n",
      "         [0.9569, 0.9647, 0.9686,  ..., 0.5569, 0.5294, 0.4980],\n",
      "         [0.9569, 0.9686, 0.9765,  ..., 0.5490, 0.4784, 0.4667]],\n",
      "\n",
      "        [[0.4314, 0.4588, 0.6588,  ..., 0.8667, 0.8941, 0.9373],\n",
      "         [0.3765, 0.4706, 0.6510,  ..., 0.8980, 0.9176, 0.9490],\n",
      "         [0.5451, 0.4824, 0.6275,  ..., 0.9294, 0.9373, 0.9412],\n",
      "         ...,\n",
      "         [0.6314, 0.6353, 0.6314,  ..., 0.7294, 0.6980, 0.6510],\n",
      "         [0.6314, 0.6392, 0.6392,  ..., 0.6980, 0.6667, 0.6314],\n",
      "         [0.6314, 0.6431, 0.6471,  ..., 0.6902, 0.6471, 0.6118]],\n",
      "\n",
      "        [[0.2667, 0.2902, 0.4863,  ..., 0.7490, 0.7725, 0.8157],\n",
      "         [0.3137, 0.3059, 0.4784,  ..., 0.7804, 0.8000, 0.8275],\n",
      "         [0.3804, 0.3137, 0.4588,  ..., 0.8118, 0.8196, 0.8235],\n",
      "         ...,\n",
      "         [0.6588, 0.6627, 0.6588,  ..., 0.4039, 0.4392, 0.4078],\n",
      "         [0.6627, 0.6706, 0.6667,  ..., 0.4471, 0.4235, 0.3922],\n",
      "         [0.6627, 0.6745, 0.6784,  ..., 0.4392, 0.4078, 0.3725]]])\n",
      "label:  <class 'str'> c\n",
      "image:  misc_2_marlon_kuntze_2604 type and image: <class 'torch.Tensor'> tensor([[[0.1333, 0.1333, 0.1333,  ..., 0.1059, 0.1059, 0.1098],\n",
      "         [0.1333, 0.1333, 0.1333,  ..., 0.1059, 0.1059, 0.1059],\n",
      "         [0.1373, 0.1373, 0.1373,  ..., 0.1020, 0.1020, 0.1020],\n",
      "         ...,\n",
      "         [0.2157, 0.2196, 0.2235,  ..., 0.9961, 0.9961, 0.9961],\n",
      "         [0.2196, 0.2196, 0.2235,  ..., 0.9961, 0.9961, 0.9961],\n",
      "         [0.2196, 0.2235, 0.2235,  ..., 0.9961, 0.9961, 0.9961]],\n",
      "\n",
      "        [[0.1373, 0.1373, 0.1373,  ..., 0.0902, 0.0863, 0.0863],\n",
      "         [0.1373, 0.1373, 0.1373,  ..., 0.0902, 0.0902, 0.0863],\n",
      "         [0.1373, 0.1373, 0.1373,  ..., 0.0902, 0.0902, 0.0863],\n",
      "         ...,\n",
      "         [0.2549, 0.2588, 0.2627,  ..., 0.9961, 0.9961, 0.9961],\n",
      "         [0.2588, 0.2588, 0.2627,  ..., 0.9961, 0.9961, 0.9961],\n",
      "         [0.2588, 0.2627, 0.2627,  ..., 0.9961, 0.9961, 0.9961]],\n",
      "\n",
      "        [[0.1412, 0.1412, 0.1412,  ..., 0.0627, 0.0667, 0.0667],\n",
      "         [0.1412, 0.1412, 0.1412,  ..., 0.0667, 0.0667, 0.0706],\n",
      "         [0.1412, 0.1412, 0.1412,  ..., 0.0667, 0.0706, 0.0706],\n",
      "         ...,\n",
      "         [0.2902, 0.2941, 0.2980,  ..., 0.9922, 0.9922, 0.9922],\n",
      "         [0.2941, 0.2941, 0.2980,  ..., 0.9922, 0.9922, 0.9922],\n",
      "         [0.2941, 0.2980, 0.2980,  ..., 0.9922, 0.9922, 0.9922]]])\n",
      "label:  <class 'str'> c\n",
      "image:  deafvideo_4_dsport06_4907 type and image: <class 'torch.Tensor'> tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "label:  <class 'str'> c\n",
      "image:  youtube_5_tawny_holmes_6189 type and image: <class 'torch.Tensor'> tensor([[[0.6706, 0.6784, 0.6824,  ..., 0.8118, 0.8275, 0.8667],\n",
      "         [0.6784, 0.6863, 0.6902,  ..., 0.8000, 0.8118, 0.8431],\n",
      "         [0.6863, 0.6902, 0.6941,  ..., 0.7922, 0.7961, 0.8039],\n",
      "         ...,\n",
      "         [0.7216, 0.7098, 0.7176,  ..., 0.4471, 0.4471, 0.4392],\n",
      "         [0.7176, 0.7176, 0.7255,  ..., 0.4471, 0.4471, 0.4392],\n",
      "         [0.7059, 0.7137, 0.7333,  ..., 0.4431, 0.4392, 0.4353]],\n",
      "\n",
      "        [[0.7765, 0.7843, 0.7882,  ..., 0.4745, 0.3020, 0.2745],\n",
      "         [0.7843, 0.7922, 0.7961,  ..., 0.4824, 0.3216, 0.2980],\n",
      "         [0.7843, 0.7922, 0.7961,  ..., 0.5137, 0.3882, 0.3294],\n",
      "         ...,\n",
      "         [0.3804, 0.3765, 0.3686,  ..., 0.4431, 0.4431, 0.4353],\n",
      "         [0.3804, 0.3843, 0.3765,  ..., 0.4431, 0.4431, 0.4353],\n",
      "         [0.3686, 0.3804, 0.3882,  ..., 0.4392, 0.4353, 0.4314]],\n",
      "\n",
      "        [[0.8784, 0.8863, 0.8902,  ..., 0.5333, 0.4078, 0.3882],\n",
      "         [0.8824, 0.8902, 0.8941,  ..., 0.5373, 0.4196, 0.4000],\n",
      "         [0.8824, 0.8902, 0.8902,  ..., 0.5569, 0.4627, 0.4157],\n",
      "         ...,\n",
      "         [0.3020, 0.3059, 0.3059,  ..., 0.4627, 0.4627, 0.4549],\n",
      "         [0.3020, 0.3059, 0.3098,  ..., 0.4627, 0.4627, 0.4549],\n",
      "         [0.2902, 0.2980, 0.3137,  ..., 0.4588, 0.4549, 0.4510]]])\n",
      "label:  <class 'str'> d\n",
      "image:  misc_2_marlon_kuntze_2653 type and image: <class 'torch.Tensor'> tensor([[[0.1294, 0.1294, 0.1255,  ..., 0.1059, 0.1098, 0.1137],\n",
      "         [0.1294, 0.1294, 0.1255,  ..., 0.1059, 0.1059, 0.1137],\n",
      "         [0.1294, 0.1294, 0.1255,  ..., 0.0980, 0.1020, 0.1098],\n",
      "         ...,\n",
      "         [0.2196, 0.2196, 0.2196,  ..., 0.9961, 0.9961, 0.9961],\n",
      "         [0.2235, 0.2235, 0.2235,  ..., 0.9961, 0.9961, 0.9961],\n",
      "         [0.2275, 0.2235, 0.2235,  ..., 0.9961, 0.9961, 0.9961]],\n",
      "\n",
      "        [[0.1333, 0.1333, 0.1294,  ..., 0.0941, 0.0902, 0.0902],\n",
      "         [0.1333, 0.1333, 0.1294,  ..., 0.0941, 0.0902, 0.0902],\n",
      "         [0.1333, 0.1333, 0.1294,  ..., 0.0941, 0.0902, 0.0902],\n",
      "         ...,\n",
      "         [0.2588, 0.2588, 0.2588,  ..., 0.9961, 0.9961, 0.9961],\n",
      "         [0.2627, 0.2627, 0.2627,  ..., 0.9961, 0.9961, 0.9961],\n",
      "         [0.2667, 0.2627, 0.2627,  ..., 0.9961, 0.9961, 0.9961]],\n",
      "\n",
      "        [[0.1412, 0.1412, 0.1373,  ..., 0.0588, 0.0588, 0.0667],\n",
      "         [0.1412, 0.1412, 0.1373,  ..., 0.0627, 0.0667, 0.0706],\n",
      "         [0.1412, 0.1412, 0.1373,  ..., 0.0745, 0.0745, 0.0824],\n",
      "         ...,\n",
      "         [0.2941, 0.2941, 0.2941,  ..., 0.9961, 0.9961, 0.9961],\n",
      "         [0.2980, 0.2980, 0.2980,  ..., 0.9961, 0.9961, 0.9961],\n",
      "         [0.3020, 0.2980, 0.2980,  ..., 0.9961, 0.9961, 0.9961]]])\n",
      "label:  <class 'str'> a\n",
      "image:  youtube_4_alex_abenchuchan_5467 type and image: <class 'torch.Tensor'> tensor([[[0.2118, 0.2118, 0.2118,  ..., 0.2039, 0.2000, 0.2000],\n",
      "         [0.2118, 0.2157, 0.2118,  ..., 0.2039, 0.2000, 0.2000],\n",
      "         [0.2118, 0.2118, 0.2118,  ..., 0.2039, 0.2000, 0.2000],\n",
      "         ...,\n",
      "         [0.1490, 0.1490, 0.1490,  ..., 0.1490, 0.1490, 0.1490],\n",
      "         [0.1490, 0.1490, 0.1490,  ..., 0.1490, 0.1490, 0.1490],\n",
      "         [0.1490, 0.1490, 0.1490,  ..., 0.1490, 0.1490, 0.1490]],\n",
      "\n",
      "        [[0.2118, 0.2118, 0.2118,  ..., 0.2039, 0.2000, 0.2000],\n",
      "         [0.2118, 0.2157, 0.2118,  ..., 0.2039, 0.2000, 0.2000],\n",
      "         [0.2118, 0.2118, 0.2118,  ..., 0.2039, 0.2000, 0.2000],\n",
      "         ...,\n",
      "         [0.1490, 0.1490, 0.1490,  ..., 0.1490, 0.1490, 0.1490],\n",
      "         [0.1490, 0.1490, 0.1490,  ..., 0.1490, 0.1490, 0.1490],\n",
      "         [0.1490, 0.1490, 0.1490,  ..., 0.1490, 0.1490, 0.1490]],\n",
      "\n",
      "        [[0.2118, 0.2118, 0.2039,  ..., 0.2039, 0.2000, 0.2000],\n",
      "         [0.2118, 0.2157, 0.2078,  ..., 0.2039, 0.2000, 0.2000],\n",
      "         [0.2118, 0.2118, 0.2118,  ..., 0.2039, 0.2000, 0.2000],\n",
      "         ...,\n",
      "         [0.1490, 0.1490, 0.1490,  ..., 0.1490, 0.1490, 0.1490],\n",
      "         [0.1490, 0.1490, 0.1490,  ..., 0.1451, 0.1490, 0.1490],\n",
      "         [0.1490, 0.1490, 0.1490,  ..., 0.1451, 0.1490, 0.1490]]])\n",
      "label:  <class 'str'> d\n",
      "image:  misc_1_carol_padden_1908 type and image: <class 'torch.Tensor'> tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "label:  <class 'str'> a\n",
      "image:  misc_2_aidan_mack_3201 type and image: <class 'torch.Tensor'> tensor([[[0.3922, 0.3961, 0.5725,  ..., 0.6941, 0.7137, 0.7765],\n",
      "         [0.4314, 0.3922, 0.5569,  ..., 0.7216, 0.7373, 0.7686],\n",
      "         [0.4706, 0.3765, 0.5255,  ..., 0.7451, 0.7529, 0.7451],\n",
      "         ...,\n",
      "         [0.9686, 0.9686, 0.9686,  ..., 0.6000, 0.5569, 0.5216],\n",
      "         [0.9686, 0.9686, 0.9647,  ..., 0.5843, 0.5412, 0.4588],\n",
      "         [0.9686, 0.9686, 0.9647,  ..., 0.5882, 0.4941, 0.3490]],\n",
      "\n",
      "        [[0.4745, 0.4902, 0.6824,  ..., 0.8549, 0.8784, 0.9373],\n",
      "         [0.5176, 0.4863, 0.6627,  ..., 0.8824, 0.9020, 0.9294],\n",
      "         [0.5608, 0.4706, 0.6314,  ..., 0.9098, 0.9176, 0.9059],\n",
      "         ...,\n",
      "         [0.6510, 0.6510, 0.6510,  ..., 0.7294, 0.6863, 0.6471],\n",
      "         [0.6510, 0.6510, 0.6471,  ..., 0.7059, 0.6627, 0.6275],\n",
      "         [0.6510, 0.6510, 0.6471,  ..., 0.7020, 0.6471, 0.6157]],\n",
      "\n",
      "        [[0.2941, 0.2941, 0.4627,  ..., 0.7569, 0.7647, 0.8235],\n",
      "         [0.3294, 0.2902, 0.4471,  ..., 0.7843, 0.7922, 0.8235],\n",
      "         [0.3725, 0.2745, 0.4157,  ..., 0.8118, 0.8078, 0.8039],\n",
      "         ...,\n",
      "         [0.6745, 0.6745, 0.6745,  ..., 0.3490, 0.4353, 0.4196],\n",
      "         [0.6745, 0.6745, 0.6706,  ..., 0.4706, 0.4314, 0.4039],\n",
      "         [0.6745, 0.6745, 0.6706,  ..., 0.4706, 0.4235, 0.3922]]])\n",
      "label:  <class 'str'> c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image:  deafvideo_4_dsport06_4926 type and image: <class 'torch.Tensor'> tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "label:  <class 'str'> c\n",
      "image:  misc_2_jehanne_mccullough_3898 type and image: <class 'torch.Tensor'> tensor([[[0.0039, 0.0039, 0.0000,  ..., 0.0353, 0.0353, 0.0353],\n",
      "         [0.0039, 0.0039, 0.0000,  ..., 0.0353, 0.0353, 0.0353],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0353, 0.0353, 0.0353],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0039, 0.0039],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0039, 0.0039],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0039, 0.0039]],\n",
      "\n",
      "        [[0.0039, 0.0039, 0.0000,  ..., 0.0314, 0.0314, 0.0314],\n",
      "         [0.0039, 0.0039, 0.0000,  ..., 0.0314, 0.0314, 0.0314],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0314, 0.0314, 0.0314],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0039, 0.0039],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0039, 0.0039],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0039, 0.0039]],\n",
      "\n",
      "        [[0.0039, 0.0039, 0.0000,  ..., 0.0353, 0.0353, 0.0353],\n",
      "         [0.0039, 0.0039, 0.0000,  ..., 0.0353, 0.0353, 0.0353],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0353, 0.0353, 0.0353],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0039, 0.0039],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0039, 0.0039],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0039, 0.0039]]])\n",
      "label:  <class 'str'> d\n",
      "image:  deafvideo_4_dsport06_4909 type and image: <class 'torch.Tensor'> tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "label:  <class 'str'> a\n",
      "image:  misc_2_marlon_kuntze_2473 type and image: <class 'torch.Tensor'> tensor([[[0.1294, 0.1294, 0.1294,  ..., 0.1137, 0.1137, 0.1176],\n",
      "         [0.1294, 0.1294, 0.1294,  ..., 0.1137, 0.1137, 0.1176],\n",
      "         [0.1294, 0.1294, 0.1294,  ..., 0.1098, 0.1098, 0.1137],\n",
      "         ...,\n",
      "         [0.2235, 0.2275, 0.2275,  ..., 0.9961, 0.9961, 0.9961],\n",
      "         [0.2235, 0.2235, 0.2235,  ..., 0.9961, 0.9961, 0.9961],\n",
      "         [0.2196, 0.2235, 0.2235,  ..., 0.9961, 0.9961, 0.9961]],\n",
      "\n",
      "        [[0.1333, 0.1333, 0.1333,  ..., 0.0980, 0.0941, 0.0941],\n",
      "         [0.1333, 0.1333, 0.1333,  ..., 0.0980, 0.0941, 0.0941],\n",
      "         [0.1333, 0.1333, 0.1333,  ..., 0.0941, 0.0941, 0.0902],\n",
      "         ...,\n",
      "         [0.2627, 0.2667, 0.2667,  ..., 0.9961, 0.9961, 0.9961],\n",
      "         [0.2627, 0.2627, 0.2627,  ..., 0.9961, 0.9961, 0.9961],\n",
      "         [0.2588, 0.2627, 0.2627,  ..., 0.9961, 0.9961, 0.9961]],\n",
      "\n",
      "        [[0.1412, 0.1412, 0.1412,  ..., 0.0588, 0.0588, 0.0627],\n",
      "         [0.1412, 0.1412, 0.1412,  ..., 0.0627, 0.0627, 0.0627],\n",
      "         [0.1412, 0.1412, 0.1412,  ..., 0.0667, 0.0667, 0.0706],\n",
      "         ...,\n",
      "         [0.2980, 0.3020, 0.3020,  ..., 0.9882, 0.9882, 0.9882],\n",
      "         [0.2980, 0.2980, 0.2980,  ..., 0.9882, 0.9882, 0.9882],\n",
      "         [0.2941, 0.2980, 0.2980,  ..., 0.9882, 0.9882, 0.9882]]])\n",
      "label:  <class 'str'> d\n",
      "image:  deafvideo_4_dsport06_4931 type and image: <class 'torch.Tensor'> tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "label:  <class 'str'> c\n",
      "image:  deafvideo_4_zainab_5665 type and image: <class 'torch.Tensor'> tensor([[[0.4706, 0.4667, 0.4745,  ..., 0.7529, 0.7451, 0.7490],\n",
      "         [0.4745, 0.4667, 0.4745,  ..., 0.7529, 0.7490, 0.7490],\n",
      "         [0.4784, 0.4706, 0.4706,  ..., 0.7569, 0.7490, 0.7490],\n",
      "         ...,\n",
      "         [0.7608, 0.7569, 0.7451,  ..., 0.1412, 0.1490, 0.1725],\n",
      "         [0.7647, 0.7647, 0.7451,  ..., 0.1647, 0.1608, 0.1725],\n",
      "         [0.7686, 0.7647, 0.7490,  ..., 0.1922, 0.1804, 0.1843]],\n",
      "\n",
      "        [[0.3804, 0.3922, 0.4039,  ..., 0.7137, 0.7059, 0.7020],\n",
      "         [0.3882, 0.3961, 0.4039,  ..., 0.7137, 0.7059, 0.7020],\n",
      "         [0.3922, 0.4000, 0.4039,  ..., 0.7176, 0.7098, 0.7059],\n",
      "         ...,\n",
      "         [0.6824, 0.6745, 0.6706,  ..., 0.1020, 0.1098, 0.1373],\n",
      "         [0.6863, 0.6784, 0.6706,  ..., 0.1373, 0.1255, 0.1333],\n",
      "         [0.6902, 0.6824, 0.6745,  ..., 0.1725, 0.1490, 0.1451]],\n",
      "\n",
      "        [[0.3294, 0.3176, 0.3137,  ..., 0.6706, 0.6588, 0.6549],\n",
      "         [0.3333, 0.3176, 0.3098,  ..., 0.6706, 0.6588, 0.6549],\n",
      "         [0.3294, 0.3176, 0.3059,  ..., 0.6706, 0.6627, 0.6588],\n",
      "         ...,\n",
      "         [0.6157, 0.6196, 0.6078,  ..., 0.1294, 0.1412, 0.1686],\n",
      "         [0.6235, 0.6235, 0.6118,  ..., 0.1647, 0.1569, 0.1647],\n",
      "         [0.6314, 0.6275, 0.6157,  ..., 0.1961, 0.1804, 0.1765]]])\n",
      "label:  <class 'str'> a\n",
      "image:  youtube_2_chickadee_3529 type and image: <class 'torch.Tensor'> tensor([[[0.2118, 0.2118, 0.2078,  ..., 0.8235, 0.8235, 0.8235],\n",
      "         [0.2118, 0.2118, 0.2078,  ..., 0.8235, 0.8235, 0.8235],\n",
      "         [0.2118, 0.2118, 0.2078,  ..., 0.8235, 0.8235, 0.8235],\n",
      "         ...,\n",
      "         [0.2588, 0.2667, 0.2706,  ..., 0.8275, 0.8275, 0.8275],\n",
      "         [0.2588, 0.2667, 0.2706,  ..., 0.8275, 0.8275, 0.8275],\n",
      "         [0.2588, 0.2667, 0.2745,  ..., 0.8275, 0.8275, 0.8275]],\n",
      "\n",
      "        [[0.2078, 0.2078, 0.2039,  ..., 0.8275, 0.8275, 0.8275],\n",
      "         [0.2078, 0.2078, 0.2000,  ..., 0.8275, 0.8275, 0.8275],\n",
      "         [0.2078, 0.2078, 0.2000,  ..., 0.8275, 0.8275, 0.8275],\n",
      "         ...,\n",
      "         [0.2196, 0.2235, 0.2275,  ..., 0.8353, 0.8353, 0.8353],\n",
      "         [0.2157, 0.2275, 0.2314,  ..., 0.8353, 0.8353, 0.8353],\n",
      "         [0.2157, 0.2275, 0.2314,  ..., 0.8353, 0.8353, 0.8353]],\n",
      "\n",
      "        [[0.1373, 0.1373, 0.1451,  ..., 0.8078, 0.8078, 0.8078],\n",
      "         [0.1373, 0.1373, 0.1451,  ..., 0.8078, 0.8078, 0.8078],\n",
      "         [0.1373, 0.1373, 0.1412,  ..., 0.8078, 0.8078, 0.8078],\n",
      "         ...,\n",
      "         [0.2235, 0.2314, 0.2392,  ..., 0.8314, 0.8314, 0.8314],\n",
      "         [0.2235, 0.2353, 0.2392,  ..., 0.8314, 0.8314, 0.8314],\n",
      "         [0.2275, 0.2353, 0.2431,  ..., 0.8314, 0.8314, 0.8314]]])\n",
      "label:  <class 'str'> d\n",
      "image:  youtube_1_alexandria_wailes_1870 type and image: <class 'torch.Tensor'> tensor([[[0.7647, 0.7686, 0.7686,  ..., 0.7098, 0.7020, 0.6941],\n",
      "         [0.7647, 0.7686, 0.7725,  ..., 0.7098, 0.7020, 0.6941],\n",
      "         [0.7686, 0.7686, 0.7725,  ..., 0.7059, 0.6980, 0.6941],\n",
      "         ...,\n",
      "         [0.6745, 0.6824, 0.6824,  ..., 0.3294, 0.3686, 0.3961],\n",
      "         [0.6706, 0.6784, 0.6824,  ..., 0.3020, 0.3333, 0.3569],\n",
      "         [0.6706, 0.6784, 0.6824,  ..., 0.2863, 0.3137, 0.3294]],\n",
      "\n",
      "        [[0.7490, 0.7529, 0.7529,  ..., 0.6941, 0.6863, 0.6784],\n",
      "         [0.7490, 0.7529, 0.7569,  ..., 0.6941, 0.6863, 0.6784],\n",
      "         [0.7529, 0.7529, 0.7569,  ..., 0.6902, 0.6824, 0.6784],\n",
      "         ...,\n",
      "         [0.6588, 0.6667, 0.6667,  ..., 0.1922, 0.2353, 0.2863],\n",
      "         [0.6549, 0.6627, 0.6667,  ..., 0.1569, 0.2000, 0.2392],\n",
      "         [0.6549, 0.6627, 0.6667,  ..., 0.1412, 0.1725, 0.2078]],\n",
      "\n",
      "        [[0.7059, 0.7098, 0.7098,  ..., 0.6588, 0.6510, 0.6431],\n",
      "         [0.7059, 0.7098, 0.7137,  ..., 0.6588, 0.6510, 0.6431],\n",
      "         [0.7098, 0.7098, 0.7137,  ..., 0.6549, 0.6471, 0.6431],\n",
      "         ...,\n",
      "         [0.6157, 0.6235, 0.6275,  ..., 0.1294, 0.1804, 0.2275],\n",
      "         [0.6118, 0.6196, 0.6275,  ..., 0.0980, 0.1412, 0.1804],\n",
      "         [0.6118, 0.6196, 0.6275,  ..., 0.0784, 0.1176, 0.1451]]])\n",
      "label:  <class 'str'> a\n",
      "image:  misc_1_carol_padden_1974 type and image: <class 'torch.Tensor'> tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "label:  <class 'str'> b\n",
      "image:  youtube_3_alex_abenchuchan_4313 type and image: <class 'torch.Tensor'> tensor([[[0.1608, 0.1647, 0.1686,  ..., 0.1961, 0.1922, 0.1882],\n",
      "         [0.1608, 0.1647, 0.1686,  ..., 0.1961, 0.1922, 0.1882],\n",
      "         [0.1608, 0.1647, 0.1686,  ..., 0.1922, 0.1882, 0.1882],\n",
      "         ...,\n",
      "         [0.1020, 0.1059, 0.1059,  ..., 0.1490, 0.1490, 0.1490],\n",
      "         [0.1020, 0.1059, 0.1059,  ..., 0.1490, 0.1490, 0.1490],\n",
      "         [0.1020, 0.1059, 0.1059,  ..., 0.1490, 0.1490, 0.1490]],\n",
      "\n",
      "        [[0.1608, 0.1647, 0.1686,  ..., 0.1961, 0.1922, 0.1882],\n",
      "         [0.1608, 0.1647, 0.1686,  ..., 0.1961, 0.1922, 0.1882],\n",
      "         [0.1608, 0.1647, 0.1686,  ..., 0.1922, 0.1882, 0.1882],\n",
      "         ...,\n",
      "         [0.1020, 0.1059, 0.1059,  ..., 0.1451, 0.1412, 0.1412],\n",
      "         [0.1020, 0.1059, 0.1059,  ..., 0.1451, 0.1412, 0.1412],\n",
      "         [0.1020, 0.1059, 0.1059,  ..., 0.1451, 0.1412, 0.1412]],\n",
      "\n",
      "        [[0.1608, 0.1647, 0.1686,  ..., 0.1961, 0.1922, 0.1882],\n",
      "         [0.1608, 0.1647, 0.1686,  ..., 0.1961, 0.1922, 0.1882],\n",
      "         [0.1608, 0.1647, 0.1686,  ..., 0.1922, 0.1882, 0.1882],\n",
      "         ...,\n",
      "         [0.1020, 0.1059, 0.1059,  ..., 0.1451, 0.1451, 0.1451],\n",
      "         [0.1020, 0.1059, 0.1059,  ..., 0.1451, 0.1451, 0.1490],\n",
      "         [0.1020, 0.1059, 0.1059,  ..., 0.1451, 0.1490, 0.1529]]])\n",
      "label:  <class 'str'> d\n",
      "image:  deafvideo_4_mattref2005_5202 type and image: <class 'torch.Tensor'> tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0078, 0.0078],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0078, 0.0078],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0078, 0.0078],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]])\n",
      "label:  <class 'str'> a\n",
      "image:  youtube_4_alex_abenchuchan_5403 type and image: <class 'torch.Tensor'> tensor([[[0.2000, 0.2000, 0.2039,  ..., 0.1882, 0.1882, 0.1843],\n",
      "         [0.2000, 0.2000, 0.2039,  ..., 0.1882, 0.1882, 0.1843],\n",
      "         [0.2000, 0.2000, 0.2000,  ..., 0.1882, 0.1843, 0.1882],\n",
      "         ...,\n",
      "         [0.1412, 0.1412, 0.1412,  ..., 0.1451, 0.1412, 0.1373],\n",
      "         [0.1412, 0.1412, 0.1412,  ..., 0.1451, 0.1412, 0.1373],\n",
      "         [0.1412, 0.1412, 0.1412,  ..., 0.1451, 0.1412, 0.1373]],\n",
      "\n",
      "        [[0.1961, 0.1961, 0.1961,  ..., 0.1843, 0.1843, 0.1843],\n",
      "         [0.1961, 0.1961, 0.1961,  ..., 0.1882, 0.1882, 0.1843],\n",
      "         [0.2000, 0.2000, 0.1961,  ..., 0.1882, 0.1843, 0.1882],\n",
      "         ...,\n",
      "         [0.1412, 0.1412, 0.1412,  ..., 0.1412, 0.1373, 0.1333],\n",
      "         [0.1373, 0.1373, 0.1412,  ..., 0.1412, 0.1373, 0.1333],\n",
      "         [0.1373, 0.1373, 0.1412,  ..., 0.1412, 0.1373, 0.1294]],\n",
      "\n",
      "        [[0.1961, 0.1922, 0.2000,  ..., 0.1882, 0.1882, 0.1843],\n",
      "         [0.1961, 0.1922, 0.2000,  ..., 0.1882, 0.1882, 0.1843],\n",
      "         [0.1961, 0.1922, 0.2000,  ..., 0.1882, 0.1843, 0.1882],\n",
      "         ...,\n",
      "         [0.1412, 0.1412, 0.1412,  ..., 0.1412, 0.1373, 0.1373],\n",
      "         [0.1412, 0.1412, 0.1412,  ..., 0.1412, 0.1373, 0.1373],\n",
      "         [0.1373, 0.1412, 0.1412,  ..., 0.1412, 0.1373, 0.1333]]])\n",
      "label:  <class 'str'> a\n",
      "image:  aslized_mj_bienvenu_7068 type and image: <class 'torch.Tensor'> tensor([[[0.0314, 0.0392, 0.0431,  ..., 0.0784, 0.0706, 0.0588],\n",
      "         [0.0275, 0.0353, 0.0431,  ..., 0.0902, 0.0824, 0.0706],\n",
      "         [0.0235, 0.0314, 0.0353,  ..., 0.0941, 0.0902, 0.0784],\n",
      "         ...,\n",
      "         [0.0039, 0.0118, 0.0157,  ..., 0.1059, 0.0941, 0.0824],\n",
      "         [0.0039, 0.0118, 0.0157,  ..., 0.1020, 0.0902, 0.0824],\n",
      "         [0.0039, 0.0118, 0.0157,  ..., 0.0941, 0.0902, 0.0824]],\n",
      "\n",
      "        [[0.0863, 0.0941, 0.0980,  ..., 0.1725, 0.1569, 0.1333],\n",
      "         [0.0824, 0.0902, 0.0980,  ..., 0.1843, 0.1647, 0.1412],\n",
      "         [0.0784, 0.0863, 0.0902,  ..., 0.1961, 0.1765, 0.1529],\n",
      "         ...,\n",
      "         [0.0431, 0.0510, 0.0549,  ..., 0.2314, 0.2196, 0.2078],\n",
      "         [0.0431, 0.0510, 0.0549,  ..., 0.2275, 0.2157, 0.2078],\n",
      "         [0.0431, 0.0510, 0.0549,  ..., 0.2235, 0.2157, 0.2078]],\n",
      "\n",
      "        [[0.0863, 0.0941, 0.0980,  ..., 0.2863, 0.2549, 0.2314],\n",
      "         [0.0824, 0.0902, 0.0980,  ..., 0.3059, 0.2706, 0.2431],\n",
      "         [0.0784, 0.0863, 0.0902,  ..., 0.3255, 0.2863, 0.2588],\n",
      "         ...,\n",
      "         [0.0471, 0.0549, 0.0588,  ..., 0.4118, 0.3882, 0.3647],\n",
      "         [0.0471, 0.0549, 0.0588,  ..., 0.4039, 0.3843, 0.3608],\n",
      "         [0.0471, 0.0549, 0.0588,  ..., 0.3961, 0.3804, 0.3569]]])\n",
      "label:  <class 'str'> d\n",
      "image:  deafvideo_4_taylerade_5664 type and image: <class 'torch.Tensor'> tensor([[[0.4588, 0.4667, 0.4863,  ..., 0.7608, 0.7529, 0.7529],\n",
      "         [0.4588, 0.4667, 0.4824,  ..., 0.7569, 0.7529, 0.7490],\n",
      "         [0.4627, 0.4706, 0.4863,  ..., 0.7569, 0.7490, 0.7412],\n",
      "         ...,\n",
      "         [0.7569, 0.7490, 0.7216,  ..., 0.2667, 0.2706, 0.2824],\n",
      "         [0.7608, 0.7569, 0.7373,  ..., 0.2667, 0.2745, 0.2863],\n",
      "         [0.7569, 0.7608, 0.7412,  ..., 0.2667, 0.2745, 0.2863]],\n",
      "\n",
      "        [[0.3922, 0.3922, 0.4039,  ..., 0.7137, 0.7020, 0.6980],\n",
      "         [0.3922, 0.3922, 0.4039,  ..., 0.7176, 0.7098, 0.7020],\n",
      "         [0.3922, 0.3961, 0.4078,  ..., 0.7216, 0.7137, 0.7059],\n",
      "         ...,\n",
      "         [0.6745, 0.6627, 0.6510,  ..., 0.2157, 0.2118, 0.2157],\n",
      "         [0.6784, 0.6706, 0.6627,  ..., 0.2196, 0.2157, 0.2196],\n",
      "         [0.6784, 0.6745, 0.6627,  ..., 0.2196, 0.2157, 0.2196]],\n",
      "\n",
      "        [[0.3294, 0.3255, 0.3294,  ..., 0.6745, 0.6667, 0.6627],\n",
      "         [0.3216, 0.3216, 0.3255,  ..., 0.6784, 0.6706, 0.6627],\n",
      "         [0.3176, 0.3216, 0.3294,  ..., 0.6824, 0.6745, 0.6627],\n",
      "         ...,\n",
      "         [0.6157, 0.6078, 0.5882,  ..., 0.2706, 0.2627, 0.2627],\n",
      "         [0.6196, 0.6118, 0.5961,  ..., 0.2706, 0.2667, 0.2667],\n",
      "         [0.6235, 0.6157, 0.5961,  ..., 0.2706, 0.2667, 0.2706]]])\n",
      "label:  <class 'str'> a\n",
      "image:  misc_1_carol_padden_2012 type and image: <class 'torch.Tensor'> tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "label:  <class 'str'> b\n",
      "image:  deafvideo_4_dsport06_4928 type and image: <class 'torch.Tensor'> tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "label:  <class 'str'> a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image:  youtube_4_raymond_merritt_4841 type and image: <class 'torch.Tensor'> tensor([[[0.5294, 0.5255, 0.5294,  ..., 0.3608, 0.3608, 0.3647],\n",
      "         [0.5333, 0.5294, 0.5294,  ..., 0.3608, 0.3608, 0.3686],\n",
      "         [0.5333, 0.5333, 0.5333,  ..., 0.3608, 0.3686, 0.3765],\n",
      "         ...,\n",
      "         [0.6235, 0.6235, 0.5961,  ..., 0.4157, 0.3647, 0.3725],\n",
      "         [0.6471, 0.6118, 0.5882,  ..., 0.3725, 0.3843, 0.4392],\n",
      "         [0.6588, 0.6039, 0.5216,  ..., 0.3843, 0.4510, 0.4902]],\n",
      "\n",
      "        [[0.3647, 0.3608, 0.3647,  ..., 0.2980, 0.3059, 0.3098],\n",
      "         [0.3686, 0.3647, 0.3647,  ..., 0.2980, 0.3059, 0.3137],\n",
      "         [0.3686, 0.3686, 0.3686,  ..., 0.3059, 0.3098, 0.3216],\n",
      "         ...,\n",
      "         [0.6078, 0.5843, 0.5529,  ..., 0.2118, 0.1882, 0.2118],\n",
      "         [0.6353, 0.5608, 0.5373,  ..., 0.1961, 0.2275, 0.2902],\n",
      "         [0.6314, 0.5451, 0.4627,  ..., 0.2275, 0.2863, 0.3176]],\n",
      "\n",
      "        [[0.2196, 0.2118, 0.2039,  ..., 0.2510, 0.2588, 0.2667],\n",
      "         [0.2157, 0.2118, 0.2039,  ..., 0.2510, 0.2588, 0.2706],\n",
      "         [0.2118, 0.2078, 0.2039,  ..., 0.2471, 0.2588, 0.2706],\n",
      "         ...,\n",
      "         [0.6353, 0.6118, 0.5804,  ..., 0.0745, 0.0824, 0.1059],\n",
      "         [0.6627, 0.5843, 0.5569,  ..., 0.0824, 0.1216, 0.1569],\n",
      "         [0.6588, 0.5569, 0.4784,  ..., 0.1255, 0.1529, 0.1569]]])\n",
      "label:  <class 'str'> a\n",
      "image:  deafvideo_4_mattref2005_5199 type and image: <class 'torch.Tensor'> tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "label:  <class 'str'> c\n",
      "image:  deafvideo_4_mattref2005_5169 type and image: <class 'torch.Tensor'> tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "label:  <class 'str'> a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:45\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, trainloader, valloader, num_epoch)\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def train(model, trainloader, valloader, num_epoch=10):  # Train the model\n",
    "    print(\"Start training...\")\n",
    "    trn_loss_hist = []\n",
    "    trn_acc_hist = []\n",
    "    val_acc_hist = []\n",
    "    model.train()  # Set the model to training mode\n",
    "    for i in range(num_epoch):\n",
    "        running_loss = []\n",
    "        print('-----------------Epoch = %d-----------------' % (i+1))\n",
    "        for batch, label in tqdm(trainloader):\n",
    "            batch = batch.to(device)\n",
    "            label = label.to(device)\n",
    "            optimizer.zero_grad()  # Clear gradients from the previous iteration\n",
    "            # This will call Network.forward() that you implement\n",
    "            pred = model(batch)\n",
    "            loss = criterion(pred, label)  # Calculate the loss\n",
    "            running_loss.append(loss.item())\n",
    "            loss.backward()  # Backprop gradients to all tensors in the network\n",
    "            optimizer.step()  # Update trainable weights\n",
    "        print(\"\\n Epoch {} loss:{}\".format(i+1, np.mean(running_loss)))\n",
    "\n",
    "        # Keep track of training loss, accuracy, and validation loss\n",
    "        trn_loss_hist.append(np.mean(running_loss))\n",
    "        trn_acc_hist.append(evaluate(model, trainloader))\n",
    "        print(\"\\n Evaluate on validation set...\")\n",
    "        val_acc_hist.append(evaluate(model, valloader))\n",
    "    print(\"Done!\")\n",
    "    return trn_loss_hist, trn_acc_hist, val_acc_hist\n",
    "\n",
    "\n",
    "def evaluate(model, loader):  # Evaluate accuracy on validation / test set\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    with torch.no_grad():  # Do not calculate grident to speed up computation\n",
    "        for batch, label in tqdm(loader):\n",
    "            batch = batch.to(device)\n",
    "            label = label.to(device)\n",
    "            pred = model(batch)\n",
    "            correct += (torch.argmax(pred, dim=1) == label).sum().item()\n",
    "        acc = correct/len(loader.dataset)\n",
    "        print(\"\\n Evaluation accuracy: {}\".format(acc))\n",
    "        return acc\n",
    "\n",
    "\n",
    "trn_loss_hist, trn_acc_hist, val_acc_hist = train(model, trainloader,\n",
    "                                                  valloader, num_epoch)\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Note down the evaluation accuracy on test set                        #\n",
    "##############################################################################\n",
    "print(\"\\n Evaluate on test set\")\n",
    "evaluate(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d81774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c4104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
