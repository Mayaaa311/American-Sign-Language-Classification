{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16893422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm # Displays a progress bar\n",
    "from math import sqrt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torchvision import datasets, io, models, ops, transforms, utils\n",
    "from torch.utils.data import Dataset, Subset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "# from torchvision import datasets, io, models, ops, transforms, utils\n",
    "import os\n",
    "# import data as dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dae4358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the GPU. You are good to go!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Using the GPU. You are good to go!\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"Using the CPU. Overall speed may be slowed down\")\n",
    "    device = 'cpu'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bba52881",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('output.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9204605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>url</th>\n",
       "      <th>start_time</th>\n",
       "      <th>number_of_frames</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>label_proc</th>\n",
       "      <th>label_raw</th>\n",
       "      <th>label_notes</th>\n",
       "      <th>partition</th>\n",
       "      <th>signer</th>\n",
       "      <th>Label</th>\n",
       "      <th>start_l</th>\n",
       "      <th>end_l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1800</td>\n",
       "      <td>deafvideo_4/mattref2005_5169</td>\n",
       "      <td>http://www.deafvideo.tv/805784</td>\n",
       "      <td>0:03:20.420000</td>\n",
       "      <td>6</td>\n",
       "      <td>640</td>\n",
       "      <td>360</td>\n",
       "      <td>iat</td>\n",
       "      <td>iat</td>\n",
       "      <td>iat?</td>\n",
       "      <td>train</td>\n",
       "      <td>37</td>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                      filename                             url  \\\n",
       "23        1800  deafvideo_4/mattref2005_5169  http://www.deafvideo.tv/805784   \n",
       "\n",
       "        start_time  number_of_frames  width  height label_proc label_raw  \\\n",
       "23  0:03:20.420000                 6    640     360        iat       iat   \n",
       "\n",
       "   label_notes partition  signer Label  start_l  end_l  \n",
       "23        iat?     train      37     a        2      3  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[df['partition']=='dev']\n",
    "df[df['filename']=='deafvideo_4/mattref2005_5169']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec47fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_exists(filename):\n",
    "    folders = ['avg_dev', 'avg_test', 'avg_train']\n",
    "    filename = filename.replace('/','_')+\".png\"\n",
    "    # print(filename)\n",
    "    for folder in folders:\n",
    "        if os.path.exists(os.path.join(folder, filename)):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "class HandSignDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, partition, transform=None):\n",
    "        self.df = pd.read_csv(csv_file, delimiter=';')\n",
    "        self.df = self.df[self.df['partition'] == partition]\n",
    "        self.df = self.df[self.df['filename'].apply(file_exists)]\n",
    "\n",
    "    # define a function to check if a file exists in any of the folders\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.df.iloc[idx, self.df.columns.get_loc('filename')]\n",
    "        filename_img = self.df.iloc[idx, self.df.columns.get_loc('filename')].replace('/','_')\n",
    "        label = self.df.iloc[idx, self.df.columns.get_loc('Label')]\n",
    "        label = ord(label) - 97\n",
    "        label = torch.tensor(label).long()\n",
    "        # if label != 'a' and label != 'b' and label != 'c' and label != 'd':\n",
    "        #     print(label)\n",
    "        image_path = os.path.join(self.root_dir, filename_img+\".png\")\n",
    "        bbox_path = os.path.join(\"BBox\", filename, \"0000.txt\")\n",
    "        \n",
    "        try:\n",
    "            with open(bbox_path) as f:\n",
    "                bbox_info = f.readline().split(',')\n",
    "                print(\"bbox_info\",bbox_info)\n",
    "            x0, y0, x1, y1, _ = bbox_info\n",
    "            x0, y0, x1, y1 = int(x0), int(y0), int(x1), int(y1)\n",
    "\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = image.crop((x0, y0, x1, y1))\n",
    "        except FileNotFoundError:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        if(np.sum(image)==0):\n",
    "            print('ALL ZERO')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # print('image name: ',filename_img,\" | shape:\",image.shape,\" | label: \",label)\n",
    "#         utils.save_image(img, f\"/ImageOutput/{filename_img}_T.png\")\n",
    "        # img1 = img1.numpy() # TypeError: tensor or list of tensors expected, got <class 'numpy.ndarray'>\n",
    "#         save_image(img, filename +'_T.png')\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d6e6f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation(s) to be applied to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((360, 640)),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                          std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Call the create_dataset function to create a PyTorch dataset\n",
    "test_dataset = HandSignDataset(csv_file='output.csv', root_dir='avg_test', partition='test',transform=transform)\n",
    "train_dataset = HandSignDataset(csv_file='output.csv', root_dir='avg_train', partition='train',transform=transform)\n",
    "val_dataset = HandSignDataset(csv_file='output.csv', root_dir='avg_dev', partition='dev',transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0400812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# Create a data loader for the dataset\n",
    "\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, collate_fn=custom_collate)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51ca7db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in trainloader:\n",
    "    img = batch[0]\n",
    "    label = batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8d9134b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "false INTERNAL ASSERT FAILED at \"../c10/cuda/CUDAGraphsC10Utils.h\":73, please report a bug to PyTorch. Unknown CUDA graph CaptureStatus32765",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 68\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[39m# print(\"final shape: \", z.shape)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m         \u001b[39mreturn\u001b[39;00m z\n\u001b[0;32m---> 68\u001b[0m model \u001b[39m=\u001b[39m Network()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     69\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss() \u001b[39m# Specify the loss layer\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mYour network:\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 23\u001b[0m, in \u001b[0;36mNetwork.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_2 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(in_features\u001b[39m=\u001b[39m\u001b[39m800\u001b[39m, out_features\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_3 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(in_features\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m, out_features\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_weights()\n",
      "Cell \u001b[0;32mIn[21], line 30\u001b[0m, in \u001b[0;36mNetwork.init_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minit_weights\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     28\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Initialize all model parameters (weights and biases) in all layers to desired distributions\"\"\"\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     torch\u001b[39m.\u001b[39;49mmanual_seed(\u001b[39m42\u001b[39;49m)\n\u001b[1;32m     32\u001b[0m     \u001b[39mfor\u001b[39;00m conv \u001b[39min\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3]:\n\u001b[1;32m     33\u001b[0m         C_in \u001b[39m=\u001b[39m conv\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/graph/lib/python3.8/site-packages/torch/random.py:40\u001b[0m, in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcuda\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_is_in_bad_fork():\n\u001b[0;32m---> 40\u001b[0m     torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mmanual_seed_all(seed)\n\u001b[1;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m default_generator\u001b[39m.\u001b[39mmanual_seed(seed)\n",
      "File \u001b[0;32m/opt/conda/envs/graph/lib/python3.8/site-packages/torch/cuda/random.py:113\u001b[0m, in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    110\u001b[0m         default_generator \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdefault_generators[i]\n\u001b[1;32m    111\u001b[0m         default_generator\u001b[39m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m--> 113\u001b[0m _lazy_call(cb, seed_all\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/graph/lib/python3.8/site-packages/torch/cuda/__init__.py:165\u001b[0m, in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_lazy_call\u001b[39m(callable, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    164\u001b[0m     \u001b[39mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 165\u001b[0m         callable()\n\u001b[1;32m    166\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    167\u001b[0m         \u001b[39m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[1;32m    168\u001b[0m         \u001b[39m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[1;32m    169\u001b[0m         \u001b[39m# else here if this ends up being important.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m         \u001b[39mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "File \u001b[0;32m/opt/conda/envs/graph/lib/python3.8/site-packages/torch/cuda/random.py:111\u001b[0m, in \u001b[0;36mmanual_seed_all.<locals>.cb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(device_count()):\n\u001b[1;32m    110\u001b[0m     default_generator \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdefault_generators[i]\n\u001b[0;32m--> 111\u001b[0m     default_generator\u001b[39m.\u001b[39;49mmanual_seed(seed)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: false INTERNAL ASSERT FAILED at \"../c10/cuda/CUDAGraphsC10Utils.h\":73, please report a bug to PyTorch. Unknown CUDA graph CaptureStatus32765"
     ]
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ##############################################################################\n",
    "        # TODO: Design your own network, define layers here.                          #\n",
    "        # Here We provide a sample of two-layer fc network from HW4 Part3.           #\n",
    "        # Your solution, however, should contain convolutional layers.               #\n",
    "        # Refer to PyTorch documentations of torch.nn to pick your layers.           #\n",
    "        # (https://pytorch.org/docs/stable/nn.html)                                  #\n",
    "        # Some common choices: Linear, Conv2d, ReLU, MaxPool2d, AvgPool2d, Dropout   #\n",
    "        # If you have many layers, use nn.Sequential() to simplify your code         #\n",
    "        ##############################################################################\n",
    "        # from 28x28 input image to hidden layer of size 256\n",
    "        # self.fc1 = nn.Linear(28*28, 8) \n",
    "        self.conv1 = nn.Conv2d(in_channels = 3,out_channels = 16,padding = 2, kernel_size = (5,5),stride = (2,2))\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 16,out_channels = 64, padding = 2,kernel_size = (5,5),stride = (2,2))\n",
    "        self.conv3 = nn.Conv2d(in_channels = 64,out_channels = 8, padding = 2,kernel_size = (5,5),stride = (2,2))\n",
    "        self.fc_1 = nn.Linear(in_features = 1760, out_features =800) \n",
    "        self.fc_2 = nn.Linear(in_features=800, out_features=200)\n",
    "        self.fc_3 = nn.Linear(in_features=200, out_features=4)\n",
    "        \n",
    "        self.init_weights()\n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize all model parameters (weights and biases) in all layers to desired distributions\"\"\"\n",
    "\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        for conv in [self.conv1, self.conv2, self.conv3]:\n",
    "            C_in = conv.weight.size(1)\n",
    "            nn.init.normal_(conv.weight, 0.0, 1 / sqrt(5 * 5 * C_in))\n",
    "            nn.init.constant_(conv.bias, 0.0)\n",
    "\n",
    "        ## TODO: initialize the parameters for [self.fc_1]\n",
    "\n",
    "        nn.init.normal_(self.fc_1.weight, 0.0, sqrt(1/self.fc_1.weight.size(1)))\n",
    "        nn.init.constant_(self.fc_1.bias, 0.0)\n",
    "        ##\n",
    "\n",
    "    def forward(self, x):\n",
    "        ##############################################################################\n",
    "        # TODO: Design your own network, implement forward pass here                 # \n",
    "        ##############################################################################\n",
    "        \n",
    "        N, C, H, W = x.shape\n",
    "\n",
    "        ## TODO: forward pass\n",
    "        z= self.pool(F.relu(self.conv1(x)))\n",
    "        z= self.pool(F.relu(self.conv2(z)))\n",
    "        z = F.relu(self.conv3(z))\n",
    "\n",
    "        # z=z.permute(*torch.arange(z.ndim - 1, -1, -1))\n",
    "        # print(\"before flatten: \", z.shape)\n",
    "        z=torch.flatten(z, start_dim=1)\n",
    "\n",
    "        # print(\"after resize: \",z.shape) # 32 x 1760\n",
    "        z = self.fc_1(z)\n",
    "        z = self.fc_2(z)\n",
    "        z = self.fc_3(z)\n",
    "        # print(\"final shape: \", z.shape)\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "model = Network().to(device)\n",
    "criterion = nn.CrossEntropyLoss() # Specify the loss layer\n",
    "print('Your network:')\n",
    "# print(summary(model, (3,28,28), device=device)) # visualize your model\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Modify the lines below to experiment with different optimizers,      #\n",
    "# parameters (such as learning rate) and number of epochs.                   #\n",
    "##############################################################################\n",
    "# Set up optimization hyperparameters\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-3\n",
    "num_epoch = 20  # TODO: Choose an appropriate number of training epochs\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                       weight_decay=weight_decay) # Try different optimizers\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87118e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/graph/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/graph/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "\n",
    "# Load the ResNet-18 model pretrained on ImageNet\n",
    "resnet18 = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2d3b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((360, 640)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Call the create_dataset function to create a PyTorch dataset\n",
    "test_dataset = HandSignDataset(csv_file='output.csv', root_dir='avg_test', partition='test',transform=transform)\n",
    "train_dataset = HandSignDataset(csv_file='output.csv', root_dir='avg_train', partition='train',transform=transform)\n",
    "val_dataset = HandSignDataset(csv_file='output.csv', root_dir='avg_dev', partition='dev',transform=transform)\n",
    "\n",
    "batch_size = 32\n",
    "# Create a data loader for the dataset\n",
    "\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, collate_fn=custom_collate)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-3\n",
    "num_epoch = 10  # TODO: Choose an appropriate number of training epochs\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                       weight_decay=weight_decay) # Try different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55421ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "-----------------Epoch = 1-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:52\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:24\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, trainloader, valloader, num_epoch)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/graph/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/graph/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def train(model, trainloader, valloader, num_epoch=1):  # Train the model\n",
    "    print(\"Start training...\")\n",
    "    trn_loss_hist = []\n",
    "    trn_acc_hist = []\n",
    "    val_acc_hist = []\n",
    "    model.train()  # Set the model to training mode\n",
    "    for i in range(num_epoch):\n",
    "        running_loss = []\n",
    "        print('-----------------Epoch = %d-----------------' % (i+1))\n",
    "        for batch, label in trainloader:\n",
    "            # print('batch: ', len(batch))\n",
    "            # print(\"type: \", type(batch))\n",
    "            batch = batch.to(device)\n",
    "            # print(batch.shape)\n",
    "            # print('batch:' , lenbatch)\n",
    "            # print(\"label: \", label)\n",
    "            label = label.to(device)\n",
    "            # print('3')\n",
    "            optimizer.zero_grad()  # Clear gradients from the previous iteration\n",
    "            # This will call Network.forward() that you implement\n",
    "            pred = model(batch)\n",
    "            loss = criterion(pred, label)  # Calculate the loss\n",
    "            running_loss.append(loss.item())\n",
    "            loss.backward()  # Backprop gradients to all tensors in the network\n",
    "            optimizer.step()  # Update trainable weights\n",
    "        print(\"\\n Epoch {} loss:{}\".format(i+1, np.mean(running_loss)))\n",
    "\n",
    "        # Keep track of training loss, accuracy, and validation loss\n",
    "        trn_loss_hist.append(np.mean(running_loss))\n",
    "        trn_acc_hist.append(evaluate(model, trainloader))\n",
    "        print(\"\\n Evaluate on validation set...\")\n",
    "        val_acc_hist.append(evaluate(model, valloader))\n",
    "    print(\"Done!\")\n",
    "    return trn_loss_hist, trn_acc_hist, val_acc_hist\n",
    "\n",
    "\n",
    "def evaluate(model, loader):  # Evaluate accuracy on validation / test set\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    with torch.no_grad():  # Do not calculate grident to speed up computation\n",
    "        for batch, label in loader:\n",
    "            batch = batch.to(device)\n",
    "            # print(batch)\n",
    "            label = label.to(device)\n",
    "            pred = model(batch)\n",
    "            correct += (torch.argmax(pred, dim=1) == label).sum().item()\n",
    "        acc = correct/len(loader.dataset)\n",
    "        print(\"\\n Evaluation accuracy: {}\".format(acc))\n",
    "        return acc\n",
    "\n",
    "\n",
    "trn_loss_hist, trn_acc_hist, val_acc_hist = train(model, trainloader,\n",
    "                                                  valloader, num_epoch)\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Note down the evaluation accuracy on test set                        #\n",
    "##############################################################################\n",
    "print(\"\\n Evaluate on test set\")\n",
    "evaluate(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e369699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('b') - 97  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
